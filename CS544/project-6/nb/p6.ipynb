{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd077541-7444-41e5-8993-6998b2a8628b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address       Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  192.168.16.2  218.04 KiB  16      100.0%            9b9cc425-26d0-4067-92d0-2b0472216ba2  rack1\n",
      "UN  192.168.16.3  223.2 KiB   16      100.0%            8b02a33d-2598-4ee4-8e81-0fda568a2231  rack1\n",
      "UN  192.168.16.4  209.17 KiB  16      100.0%            2a061f01-8fd5-4d8e-9967-1b69efb02e7e  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53fcc9fa-b138-4b11-846e-8a81e7274e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87c2962-da81-4da4-8343-28db4eb55179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f412498d870>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"DROP KEYSPACE IF EXISTS weather\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "CREATE KEYSPACE weather\n",
    "WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 3}\n",
    "\"\"\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "CREATE TYPE weather.station_record (\n",
    "    tmin INT,\n",
    "    tmax INT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "CREATE TABLE weather.stations (\n",
    "    id TEXT,\n",
    "    name TEXT STATIC,\n",
    "    date DATE,\n",
    "    record weather.station_record,\n",
    "    PRIMARY KEY ((id), date)\n",
    ") WITH CLUSTERING ORDER BY (date ASC)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4fbee8e-084c-4212-b4ab-dc1516d0c275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE weather.stations (\\n    id text,\\n    date date,\\n    name text static,\\n    record station_record,\\n    PRIMARY KEY (id, date)\\n) WITH CLUSTERING ORDER BY (date ASC)\\n    AND additional_write_policy = '99p'\\n    AND bloom_filter_fp_chance = 0.01\\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\\n    AND cdc = false\\n    AND comment = ''\\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\\n    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\\n    AND memtable = 'default'\\n    AND crc_check_chance = 1.0\\n    AND default_time_to_live = 0\\n    AND extensions = {}\\n    AND gc_grace_seconds = 864000\\n    AND max_index_interval = 2048\\n    AND memtable_flush_period_in_ms = 0\\n    AND min_index_interval = 128\\n    AND read_repair = 'BLOCKING'\\n    AND speculative_retry = '99p';\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "# What is the Schema of stations?\n",
    "cass.execute(\"describe table weather.stations\").one().create_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a02d49-343e-497c-a28a-3496e8173aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f37a8a9f-cf5d-4c05-a301-a90b8b9ed48a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 1182ms :: artifacts dl 39ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   0   ||   18  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f37a8a9f-cf5d-4c05-a301-a90b8b9ed48a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 18 already retrieved (0kB/34ms)\n",
      "23/11/15 22:51:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "044842d6-d485-4a7a-9ba9-452905a27472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "df = spark.read.format(\"text\").load(\"/nb/ghcnd-stations.txt\")\n",
    "\n",
    "df_extracted = df.withColumn(\"id\", expr(\"substring(value, 1, 11)\")) \\\n",
    "                 .withColumn(\"state\", expr(\"substring(value, 39, 2)\")) \\\n",
    "                 .withColumn(\"name\", expr(\"substring(value, 42, 30)\")) \\\n",
    "                 .where(col(\"state\") == 'WI')\n",
    "\n",
    "wi_stations_data = df_extracted.collect()\n",
    "\n",
    "for row in wi_stations_data:\n",
    "    cass.execute(\"\"\"\n",
    "    INSERT INTO weather.stations (id, name)\n",
    "    VALUES (%s, %s)\n",
    "    \"\"\", (row.id, row.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b30fd447-4004-4ba7-b4f1-162ce730c50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1313"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query_result = cass.execute(\"SELECT COUNT(*) FROM weather.stations\")\n",
    "\n",
    "df = pd.DataFrame(list(query_result))\n",
    "\n",
    "count_value = df.iloc[0]['count']\n",
    "\n",
    "count_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c854cee-1ef1-4dfd-8ca0-2731ffdf9a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MADISON DANE CO RGNL AP       '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "# what is the name corresponding to station ID USW00014837?\n",
    "query_result2 = cass.execute(\"SELECT name FROM weather.stations WHERE id = 'USW00014837'\")\n",
    "\n",
    "df2 = pd.DataFrame(list(query_result2))\n",
    "\n",
    "if not df2.empty:\n",
    "    stat_name = df2.iloc[0]['name']\n",
    "else:\n",
    "    stat_name = \"No data found for station ID 'USW00014837'\"\n",
    "\n",
    "stat_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ab8930d-cfd4-44f8-90b0-17450f7e06ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "# what is the token for the USC00470273 station?\n",
    "query_result3 = cass.execute(\"SELECT token(id) FROM weather.stations WHERE id = 'USC00470273'\")\n",
    "\n",
    "df3 = pd.DataFrame(list(query_result3))\n",
    "\n",
    "if not df3.empty:\n",
    "    stat_token = df3.iloc[0]['system_token_id']\n",
    "else:\n",
    "    stat_token = \"No data found for station ID 'USC00470273'\"\n",
    "\n",
    "stat_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "748580fd-16bc-4a38-935e-ba29d1602df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8656263686293193281"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "# what is the first vnode token in the ring following the token for USC00470273?\n",
    "\n",
    "import subprocess\n",
    "from subprocess import check_output\n",
    "\n",
    "query_result4 = cass.execute(\"SELECT token(id) FROM weather.stations WHERE id = 'USC00470273'\")\n",
    "\n",
    "df4 = pd.DataFrame(list(query_result4))\n",
    "\n",
    "if not df4.empty:\n",
    "    stat_token2 = df4.iloc[0]['system_token_id']\n",
    "else:\n",
    "    stat_token2 = \"No data found for station ID 'USC00470273'\"\n",
    "\n",
    "stat_token2\n",
    "\n",
    "output = subprocess.check_output([\"nodetool\", \"ring\"]).decode('utf-8')\n",
    "\n",
    "vnodes = []\n",
    "for line in output.split(\"\\n\"):\n",
    "    parts = line.split()\n",
    "    if len(parts) > 0:\n",
    "        try:\n",
    "            token = int(parts[-1])\n",
    "            vnodes.append(token)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "vnodes.sort()\n",
    "\n",
    "curr_token = None\n",
    "for vnode in vnodes:\n",
    "    if stat_token2 < vnode:\n",
    "        curr_token = vnode\n",
    "        break\n",
    "\n",
    "if curr_token is None:\n",
    "    curr_token = vnodes[0]\n",
    "\n",
    "curr_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa1d9bd-5584-4f29-9edc-36ca482a8d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+------+\n",
      "|    station|      date|  tmax|  tmin|\n",
      "+-----------+----------+------+------+\n",
      "|USW00014898|2022-01-07| -71.0|-166.0|\n",
      "|USW00014839|2022-05-23| 150.0|  83.0|\n",
      "|USW00014839|2022-09-24| 194.0| 117.0|\n",
      "|USR0000WDDG|2022-11-30| -39.0|-106.0|\n",
      "|USR0000WDDG|2022-01-19| -56.0|-178.0|\n",
      "|USW00014839|2022-05-29| 261.0| 139.0|\n",
      "|USW00014839|2022-10-19|  83.0|  11.0|\n",
      "|USW00014837|2022-02-22| -38.0| -88.0|\n",
      "|USR0000WDDG|2022-02-02|-106.0|-150.0|\n",
      "|USW00014839|2022-09-17| 294.0| 200.0|\n",
      "|USW00014839|2022-07-08| 222.0| 189.0|\n",
      "|USW00014839|2022-04-27|  39.0|   0.0|\n",
      "|USW00014837|2022-06-24| 322.0| 200.0|\n",
      "|USW00014898|2022-01-29| -60.0|-116.0|\n",
      "|USW00014839|2022-07-15| 233.0| 156.0|\n",
      "|USR0000WDDG|2022-01-30| -33.0|-117.0|\n",
      "|USR0000WDDG|2022-02-24| -61.0|-128.0|\n",
      "|USR0000WDDG|2022-04-14|  50.0| -17.0|\n",
      "|USW00014898|2022-07-28| 256.0| 156.0|\n",
      "|USW00014837|2022-08-02| 306.0| 150.0|\n",
      "+-----------+----------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"p6\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df = spark.read.parquet('records.parquet')\n",
    "\n",
    "# Filter out rows that are not tmin or tmax\n",
    "filtered_df = df.filter(df['element'].isin('TMIN', 'TMAX'))\n",
    "\n",
    "# Pivot the data to get tmin and tmax in separate columns\n",
    "pivot_df = filtered_df.groupBy('station', 'date').pivot('element').agg(max(col('value')))\n",
    "\n",
    "# Rename the pivoted columns for clarity\n",
    "transformed_df = pivot_df.withColumnRenamed('TMIN', 'tmin').withColumnRenamed('TMAX', 'tmax')\n",
    "\n",
    "from pyspark.sql.functions import col, concat_ws, expr\n",
    "\n",
    "# Convert the 'date' string format from 'yyyyMMdd' to 'yyyy-MM-dd'\n",
    "transformed_df = transformed_df.withColumn('date', \n",
    "    concat_ws('-', \n",
    "        expr(\"substring(date, 1, 4)\"),  # Extract the year\n",
    "        expr(\"substring(date, 5, 2)\"),  # Extract the month\n",
    "        expr(\"substring(date, 7, 2)\")   # Extract the day\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "transformed_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfecf886-3971-45c7-8010-110e51cb5d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import station_pb2, station_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('p6-db-1:5440')\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "rows = transformed_df.collect()\n",
    "\n",
    "# Loop and make gRPC calls\n",
    "for row in rows:\n",
    "    request = station_pb2.RecordTempsRequest(\n",
    "        station=row['station'],\n",
    "        date=row['date'], \n",
    "        tmin=int(row['tmin']) if row['tmin'] is not None else 0,\n",
    "        tmax=int(row['tmax']) if row['tmax'] is not None else 0\n",
    "    )\n",
    "    try:\n",
    "        response = stub.RecordTemps(request)\n",
    "        if response.error:\n",
    "            print(f\"Error recording temps for station {row['station']} on {row['date']}: {response.error}\")\n",
    "    except grpc.RpcError as e:\n",
    "        print(f\"gRPC error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbfeedaf-85bb-46dc-8231-bce41b751b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "max_temp = 0\n",
    "try:\n",
    "    response = stub.StationMax(station_pb2.StationMaxRequest(station='USW00014837'))\n",
    "    if response.error:\n",
    "        print(f\"Error: {response.error}\")\n",
    "    else:\n",
    "        max_temp = response.tmax\n",
    "except grpc.RpcError as e:\n",
    "    print(f\"gRPC error occurred: {e}\")\n",
    "\n",
    "max_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09a1a39c-6660-4df9-b8a2-9f362d3f1166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stations', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "# Read data from the Cassandra 'stations' table\n",
    "stations_df = (spark.read.format(\"org.apache.spark.sql.cassandra\")\n",
    "               .option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\")\n",
    "               .option(\"keyspace\", \"weather\")\n",
    "               .option(\"table\", \"stations\")\n",
    "               .load())\n",
    "\n",
    "# Create a temporary view named 'stations'\n",
    "stations_df.createOrReplaceTempView(\"stations\")\n",
    "\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b533a50d-3f5a-4b32-bf47-924d28aedbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, date: date, record: struct<tmin:int,tmax:int>, name: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f68af91-e18a-47dd-be80-831f5f8f1158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'USW00014837': 105.62739726027397,\n",
       " 'USR0000WDDG': 102.06849315068493,\n",
       " 'USW00014839': 89.6986301369863,\n",
       " 'USW00014898': 102.93698630136986}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 22:53:26 WARN ChannelPool: [s0|p6-db-2/192.168.16.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=58947d90-4a4d-4339-9b4a-47f442ed1225, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700088669838}): failed to send request (java.nio.channels.NotYetConnectedException))\n",
      "23/11/15 22:53:33 WARN ChannelPool: [s0|p6-db-2/192.168.16.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=58947d90-4a4d-4339-9b4a-47f442ed1225, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700088669838}): failed to send request (java.nio.channels.NotYetConnectedException))\n",
      "23/11/15 22:53:41 WARN ChannelPool: [s0|p6-db-2/192.168.16.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=58947d90-4a4d-4339-9b4a-47f442ed1225, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700088669838}): failed to send request (java.nio.channels.NotYetConnectedException))\n"
     ]
    }
   ],
   "source": [
    "#q7\n",
    "\n",
    "averages = spark.sql(\"\"\"\n",
    "SELECT id, AVG(record.tmax-record.tmin) AS average\n",
    "FROM stations\n",
    "WHERE record is not NULL\n",
    "GROUP BY id\n",
    "\"\"\").collect()\n",
    "\n",
    "dict = {}\n",
    "\n",
    "for avg in averages:\n",
    "    dict[avg[\"id\"]] = avg[\"average\"]\n",
    "\n",
    "dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7bdac0-5d87-41b7-8845-aa867034f816",
   "metadata": {},
   "source": [
    "## PART 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a0cb59b-ad28-4d4a-a81f-072f08e5e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address       Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "DN  192.168.16.2  285.53 KiB  16      100.0%            9b9cc425-26d0-4067-92d0-2b0472216ba2  rack1\n",
      "UN  192.168.16.3  291.13 KiB  16      100.0%            8b02a33d-2598-4ee4-8e81-0fda568a2231  rack1\n",
      "UN  192.168.16.4  276.67 KiB  16      100.0%            2a061f01-8fd5-4d8e-9967-1b69efb02e7e  rack1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 22:53:53 WARN ChannelPool: [s0|p6-db-2/192.168.16.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=58947d90-4a4d-4339-9b4a-47f442ed1225, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700088669838}): failed to send request (java.nio.channels.NotYetConnectedException))\n",
      "23/11/15 22:54:08 WARN ChannelPool: [s0|p6-db-2/192.168.16.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=58947d90-4a4d-4339-9b4a-47f442ed1225, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700088669838}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "23/11/15 22:54:46 WARN ChannelPool: [s0|p6-db-2/192.168.16.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=58947d90-4a4d-4339-9b4a-47f442ed1225, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700088669838}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6de04a22-9d9f-42ac-9939-726a22c3a4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address       Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "DN  192.168.16.2  285.53 KiB  16      100.0%            9b9cc425-26d0-4067-92d0-2b0472216ba2  rack1\n",
      "UN  192.168.16.3  291.13 KiB  16      100.0%            8b02a33d-2598-4ee4-8e81-0fda568a2231  rack1\n",
      "UN  192.168.16.4  276.67 KiB  16      100.0%            2a061f01-8fd5-4d8e-9967-1b69efb02e7e  rack1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 22:55:46 WARN ChannelPool: [s0|p6-db-2/192.168.16.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=58947d90-4a4d-4339-9b4a-47f442ed1225, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700088669838}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "23/11/15 22:56:48 WARN ChannelPool: [s0|p6-db-2/192.168.16.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=58947d90-4a4d-4339-9b4a-47f442ed1225, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700088669838}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed17f3ee-22a3-424f-932a-161a9f77a73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gRPC error occurred: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.INTERNAL\n",
      "\tdetails = \"need 3 replicas, but only have 2\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:192.168.16.3:5440 {created_time:\"2023-11-15T22:56:52.429969731+00:00\", grpc_status:13, grpc_message:\"need 3 replicas, but only have 2\"}\"\n",
      ">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 22:57:48 WARN ChannelPool: [s0|p6-db-2/192.168.16.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=58947d90-4a4d-4339-9b4a-47f442ed1225, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700088669838}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "23/11/15 22:58:51 WARN ChannelPool: [s0|p6-db-2/192.168.16.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=58947d90-4a4d-4339-9b4a-47f442ed1225, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700088669838}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n"
     ]
    }
   ],
   "source": [
    "#q9\n",
    "station_max_request = station_pb2.StationMaxRequest(station='USW00014837')\n",
    "\n",
    "try:\n",
    "    response = stub.StationMax(station_max_request)\n",
    "except grpc.RpcError as e:\n",
    "    print(f\"gRPC error occurred: {e}\")\n",
    "\n",
    "response.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c8dd966-3839-493c-8ef6-686fc4010345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q10\n",
    "sample_station = \"SAMPLE_STATION_ID\"\n",
    "sample_date = \"2023-11-15\"  \n",
    "sample_tmin = 10  \n",
    "sample_tmax = 20 \n",
    "\n",
    "sample_request = station_pb2.RecordTempsRequest(\n",
    "    station=sample_station,\n",
    "    date=sample_date,\n",
    "    tmin=sample_tmin,\n",
    "    tmax=sample_tmax\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = stub.RecordTemps(sample_request)\n",
    "except grpc.RpcError as e:\n",
    "    print(f\"gRPC error occurred: {e}\")\n",
    "\n",
    "response.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8878d7-0b1a-4989-bfd2-72ec9bbb2b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
