# Estimate the probability
probability_estimate <- estimate_probability(n, lambda, epsilon, M)
probability_estimate
running_var <- function(vec) {
n <- length(vec)
running_mean <- cumsum(vec) / seq_along(vec)
running_mean_sq <- cumsum(vec^2) / seq_along(vec)
running_var <- running_mean_sq - running_mean^2
return(running_var)
}
running_var <- function(vec) {
n <- length(vec)
running_mean <- cumsum(vec) / seq_along(vec)
running_mean_sq <- cumsum(vec^2) / seq_along(vec)
running_var <- running_mean_sq - running_mean^2
return(running_var)
}
pois_draws <- rpois(n=1000, lambda=5.0)
run_var <- running_var(pois_draws)
# Plot the running variance
plot(run_var, type = "l", main = "Running Variance of Poisson Draws", xlab = "Sample Size", ylab = "Running Variance")
abline(h = lambda, col = "red")
# Generate samples
samples <- rnorm(1000, mean = 0, sd = 1)
# Compute running variance
run_var <- running_var(samples)
# Plot the running variance
plot(run_var, type = 'l', xlab = "Sample Size", ylab = "Running Variance",
main = "Running Variance of a Standard Normal Distribution")
abline(h = 1, col = 'red')
# Generate samples
samples <- rnorm(10000, mean = 0, sd = 1)
# Compute running variance
run_var <- running_var(samples)
# Plot the running variance
plot(run_var, type = 'l', xlab = "Sample Size", ylab = "Running Variance",
main = "Running Variance of a Standard Normal Distribution")
abline(h = 1, col = 'red')
knitr::opts_chunk$set(echo = TRUE)
download.file('https://kdlevin-uwstat.github.io/STAT340-Fall2021/hw/03/mule_kicks.csv', destfile='mule_kicks.csv')
mule_kicks = read.csv('mule_kicks.csv', header=TRUE)
head(mule_kicks)
View(mule_kicks)
#TODO: estimate the rate parameter.
lambdahat <- mean(mule_kicks$deaths) # TODO: write code store your estimate in lambdahat.
lambdahat
?rpois
mc_resampling <- function (reps, sample_size, lambda) {
estimators <- numeric(reps)
for (i in 1:reps){
sample <- rpois(sample_size,lambda)
estimators[i] <- mean(sample)
}
return (estimators)
}
hist(mc_resampling(1e4, length(mule_kicks), lambdahat))
#TODO: code goes here.
mc_resampling <- function (reps, sample_size, lambda) {
estimators <- numeric(reps)
for (i in 1:reps){
sample <- rpois(sample_size,lambda)
estimators[i] <- mean(sample)
}
return (estimators)
}
hist(mc_resampling(1e4, length(mule_kicks$deaths), lambdahat))
#TODO: code goes here.
mc_resampling <- function (reps, sample_size, lambda) {
estimators <- numeric(reps)
for (i in 1:reps){
sample <- rpois(sample_size,lambda)
estimators[i] <- mean(sample)
}
return (estimators)
}
# Simulate the sampling distribution of the estimator
simulated_means <- mc_resampling(1e4, length(mule_kicks$deaths), lambdahat)
# Construct a 95% confidence interval from the simulated sample means
alpha <- 0.05
lower_bound <- quantile(simulated_means, alpha / 2)
upper_bound <- quantile(simulated_means, 1 - (alpha / 2))
# Display the histogram of simulated_means with the CI bounds
hist(simulated_means, main="Distribution of Simulated Sample Means with CI", xlab="Lambda Hat")
abline(v = c(lower_bound, upper_bound), col = "red", lwd = 2)
# Print the CI
cat("95% Confidence Interval for Lambda:", lower_bound, upper_bound, "\n")
#TODO: code goes here.
library(ggplot2)
# Generate a histogram of observed data
ggplot(mule_kicks, aes(x=deaths)) +
geom_histogram(binwidth=1, color="black", fill="lightblue", boundary=0) +
labs(title="Histogram of Mule Kick Deaths", x="Number of Deaths", y="Frequency") +
theme_minimal()
# Overlay the expected Poisson distribution
lambda_est <- mean(mule_kicks$deaths)
deaths_seq <- 0:max(mule_kicks$deaths)
expected_freq <- dpois(deaths_seq, lambda=lambda_est) * nrow(mule_kicks)
lines(deaths_seq, expected_freq, col="red", type="o")
library(ggplot2)
# Generate a histogram of observed data
ggplot(mule_kicks, aes(x=deaths)) +
geom_histogram(binwidth=1, color="black", fill="lightblue", boundary=0) +
labs(title="Histogram of Mule Kick Deaths", x="Number of Deaths", y="Frequency") +
theme_minimal()
# Overlay the expected Poisson distribution
lambda_est <- mean(mule_kicks$deaths)
deaths_seq <- 0:max(mule_kicks$deaths)
expected_freq <- dpois(deaths_seq, lambda=lambda_est) * nrow(mule_kicks)
lines(deaths_seq, expected_freq, col="red", type="o")
library(ggplot2)
obs_freq <- table(mule_kicks$deaths)
lambdahat <- mean(mule_kicks$deaths)
deaths_seq <- 0:max(as.integer(names(obs_freq))) # Ensure coverage of all observed deaths
expected_freq <- dpois(deaths_seq, lambdahat) * length(mule_kicks$deaths)
# Plot observed frequencies
barplot(obs_freq, names.arg = names(obs_freq), main = "Observed vs. Expected Frequencies", xlab = "Deaths", ylab = "Frequency")
# Overlay expected frequencies
lines(deaths_seq, expected_freq, col = "red", type = "o")
?rbinom
# Assume p=0.82 is truth
phat <- 0.82
desired_prob <- 0.99
min_functional <- 5
simulate_batch <- function(batch_size, p) {
rbinom(1, batch_size, p) >= min_functional
}
# Function to estimate probability for a given batch size
estimate_prob <- function(batch_size, p, simulations = 1e4) {
mean(replicate(simulations, simulate_batch(batch_size, p)))
}
batch_size <- 1
while (estimate_prob(batch_size, p) < desired_prob) {
batch_size <- batch_size + 1
}
# Assume p=0.82 is truth
phat <- 0.82
desired_prob <- 0.99
min_functional <- 5
simulate_batch <- function(batch_size, p) {
rbinom(1, batch_size, p) >= min_functional
}
# Function to estimate probability for a given batch size
estimate_prob <- function(batch_size, p, simulations = 1e4) {
mean(replicate(simulations, simulate_batch(batch_size, p)))
}
batch_size <- 1
while (estimate_prob(batch_size, phat) < desired_prob) {
batch_size <- batch_size + 1
}
batch_size
prob_get_a <- -6 + 0.05*40 + 1*3.5
prob_get_a
prob_get_a <- -6 + 0.05*40 + 1*3.5
exp(prob_get_a)
z <- -6 + 0.05*40 + 1*3.5
prob_get_a <- 1 / (1 + exp(-z))
prob_get_a
z_40 <- -6 + 0.05 * 40 + 1 * 3.5
z_41 <- -6 + 0.05 * 41 + 1 * 3.5
P_40 <- 1 / (1 + exp(-z_40))
P_41 <- 1 / (1 + exp(-z_41))
P_40
P_41
P_difference <- P_41 - P_40
P_difference
z_40 <- -6 + 0.05 * 40 + 1 * 3.5
z_41 <- -6 + 0.05 * 41 + 1 * 3.5
z_40
z_41
P_40 <- 1 / (1 + exp(-z_40))
P_41 <- 1 / (1 + exp(-z_41))
P_40
P_41
P_difference <- P_41 - P_40
P_difference
num_hours <- (log(1 - (1/0.5)) - 3.5 + 6)/0.05
num_hours
num_hours <- (-log((1/0.5)-1) - 3.5 + 6)/0.05
num_hours
#Run this code once to install the library
install.packages("palmerpenguins")
library(palmerpenguins)
?palmerpenguins
data("penguins")
penguins$Adelie <- ifelse(penguins$species == "Adelie", 1, 0)
head(penguins)
island_adelie_table <- table(penguins$island, penguins$Adelie)
island_adelie_table
island_adelie_table <- table(penguins$island, penguins$Adelie)
island_adelie_table
island_adelie_table$propotion = island_adelie_table$1 / (island_adelie_table$0 + island_adelie_table$1)
island_adelie_table <- table(penguins$island, penguins$Adelie)
island_adelie_table
df_island_adelie$Proportion_Adelie <- df_island_adelie$`1` / (df_island_adelie$`0` + df_island_adelie$`1`)
island_adelie_table <- table(penguins$island, penguins$Adelie)
island_adelie_table
df_island_adelie$Proportion_Adelie <- island_adelie_table$`1` / (island_adelie_table$`0` + island_adelie_table$`1`)
island_adelie_table <- table(penguins$island, penguins$Adelie)
island_adelie_table
df_island_adelie <- as.data.frame.matrix(island_adelie_table)
df_island_adelie$Proportion_Adelie <- df_island_adelie$`1` / (df_island_adelie$`0` + df_island_adelie$`1`)
df_island_adelie
df_island_adelie["Dream"]$`1`
df_island_adelie$`1`
df_island_adelie %>% filter("Dream")
dream_adelie <- df_island_adelie["Dream", "1"]
dream_not_adelie <- df_island_adelie["Dream", "0"]
dream_total <- dream_adelie + dream_not_adelie
prob_adelie <- dream_adelie / dream_total
prob_adelie
odds_adelie <- prob_adelie / (1 - prob_adelie)
odds_adelie
log_odds_adelie <- log(odds_adelie)
log_odds_adelie
penguins_complete <- na.omit(penguins)
adelie_model <- glm(species == "Adelie" ~ island, data = penguins_complete, family = binomial)
summary(adelie_model)
coef(adelie_model)
odds_biscoe <- exp(coef(adelie_model)["(Intercept)"])
prob_biscoe <- odds_biscoe / (1 + odds_biscoe)
prob_biscoe
coef(adelie_model)["(Intercept)"]
log_odds_dream <- coef(adelie_model)["(Intercept)"] + 1*coef(adelie_model)["islandDream"]
odds_dream <- exp(log_odds_dream)
probability_dream <- odds_dream / (1 + odds_dream)
probability_dream
log_odds_dream <- coef(adelie_model)["(Intercept)"] + 1*coef(adelie_model)["islandDream"]
odds_dream <- exp(log_odds_dream)
probability_dream <- unname(odds_dream / (1 + odds_dream))
probability_dream
penguins.complete <- penguins[complete.cases(penguins),]
adelie_bill_model <- glm(species == "Adelie" ~ bill_length_mm + bill_depth_mm,
data = penguins.complete, family = binomial)
summary(adelie_bill_model)
penguins.complete <- penguins[complete.cases(penguins),]
adelie_bill_model <- glm(species == "Adelie" ~ bill_length_mm + bill_depth_mm,
data = penguins.complete, family = binomial)
summary(adelie_bill_model)
adelie_bill_model <- glm(species == "Adelie" ~ bill_length_mm + bill_depth_mm,
data = penguins.complete, family = binomial)
summary(adelie_bill_model)
new_penguin <- data.frame(bill_length_mm = 53.1, bill_depth_mm = 22.7)
predicted_prob <- predict(adelie_bill_model, newdata = new_penguin, type = "response")
predicted_prob
24.1314 + -2.2099*53.1 + 3.9981*22.7
exp(-2.45742)
0.08565566/1+0.08565566
0.08565566/1-0.08565566
0.08565566/(1+0.08565566)
chinstrap_model <- glm(species == "Chinstrap" ~ bill_length_mm + bill_depth_mm +
flipper_length_mm + body_mass_g,
data = penguins.complete, family = binomial())
summary(chinstrap_model)
penguins.complete$predicted_prob <- predict(chinstrap_model, type = "response")
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > 0.5, 1, 0)
penguins.complete$actual_chinstrap <- ifelse(penguins.complete$species == "Chinstrap", 1, 0)
conf_matrix <- table(Predicted = penguins.complete$predicted_chinstrap, Actual = penguins.complete$actual_chinstrap)
conf_matrix
penguins.complete$predicted_prob <- predict(chinstrap_model, type = "response")
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > 0.5, 1, 0)
penguins.complete$actual_chinstrap <- ifelse(penguins.complete$species == "Chinstrap", 1, 0)
conf_matrix <- table(Predicted = penguins.complete$predicted_chinstrap, Actual = penguins.complete$actual_chinstrap)
conf_matrix
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])
type_1_error
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
power
1/265
3/68
65/68
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
# Test thresholds from 0 to 1 in small steps
thresholds <- seq(0, 1, by = 0.01)
for (thresh in thresholds) {
metrics <- calculate_metrics(penguins.complete, thresh)
if (metrics$type_1_error < 0.05) {  # Check if Type 1 error is below the desired level
if (metrics$power > best_power) {  # Check if this threshold provides a higher power
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
calculate_metrics <- function(data, threshold) {
data$predicted_chinstrap <- ifelse(data$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = data$predicted_chinstrap, Actual = data$actual_chinstrap)
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
# Test thresholds from 0 to 1 in small steps
thresholds <- seq(0, 1, by = 0.01)
for (thresh in thresholds) {
metrics <- calculate_metrics(penguins.complete, thresh)
if (metrics$type_1_error < 0.05) {  # Check if Type 1 error is below the desired level
if (metrics$power > best_power) {  # Check if this threshold provides a higher power
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
calculate_metrics <- function(threshold) {
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = penguins.complete$predicted_chinstrap, Actual = penguins.complete$actual_chinstrap)
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
# Test thresholds from 0 to 1 in small steps
thresholds <- seq(0, 1, by = 0.01)
for (thresh in thresholds) {
metrics <- calculate_metrics(penguins.complete, thresh)
if (metrics$type_1_error < 0.05) {  # Check if Type 1 error is below the desired level
if (metrics$power > best_power) {  # Check if this threshold provides a higher power
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
calculate_metrics <- function(threshold) {
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = penguins.complete$predicted_chinstrap, Actual = penguins.complete$actual_chinstrap)
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
# Test thresholds from 0 to 1 in small steps
thresholds <- seq(0, 1, by = 0.01)
for (thresh in thresholds) {
metrics <- calculate_metrics(thresh)
if (metrics$type_1_error < 0.05) {  # Check if Type 1 error is below the desired level
if (metrics$power > best_power) {  # Check if this threshold provides a higher power
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
calculate_metrics <- function(threshold) {
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = penguins.complete$predicted_chinstrap, Actual = penguins.complete$actual_chinstrap)
print(conf_matrix)
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
# Test thresholds from 0 to 1 in small steps
thresholds <- seq(0, 1, by = 0.01)
for (thresh in thresholds) {
metrics <- calculate_metrics(thresh)
if (metrics$type_1_error < 0.05) {  # Check if Type 1 error is below the desired level
if (metrics$power > best_power) {  # Check if this threshold provides a higher power
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
calculate_metrics <- function(threshold) {
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = factor(penguins.complete$predicted_chinstrap, levels = c(0, 1)), Actual = factor(penguins.complete$actual_chinstrap, levels = c(0, 1)))
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
# Test thresholds from 0 to 1 in small steps
thresholds <- seq(0, 1, by = 0.01)
for (thresh in thresholds) {
metrics <- calculate_metrics(thresh)
if (metrics$type_1_error < 0.05) {  # Check if Type 1 error is below the desired level
if (metrics$power > best_power) {  # Check if this threshold provides a higher power
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
# Output the best results
cat("Best Threshold:", best_threshold, "\n")
cat("Type 1 Error Rate:", best_type_1_error, "\n")
cat("Power (Sensitivity):", best_power, "\n")
calculate_metrics <- function(threshold) {
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = factor(penguins.complete$predicted_chinstrap, levels = c(0, 1)), Actual = factor(penguins.complete$actual_chinstrap, levels = c(0, 1)))
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
thresholds <- seq(0, 1, by = 0.001)
for (thresh in thresholds) {
metrics <- calculate_metrics(thresh)
if (metrics$type_1_error < 0.05) {
if (metrics$power > best_power) {
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
cat("Best Threshold:", best_threshold, "\n")
cat("Type 1 Error Rate:", best_type_1_error, "\n")
cat("Power (Sensitivity):", best_power, "\n")
calculate_metrics <- function(threshold) {
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = factor(penguins.complete$predicted_chinstrap, levels = c(0, 1)), Actual = factor(penguins.complete$actual_chinstrap, levels = c(0, 1)))
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
thresholds <- seq(0, 1, by = 0.001)
for (thresh in thresholds) {
metrics <- calculate_metrics(thresh)
if (metrics$type_1_error < 0.05) {
if (metrics$power > best_power) {
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
cat("Best Threshold:", best_threshold, "\n")
cat("Type 1 Error Rate:", best_type_1_error, "\n")
cat("Best Power:", best_power, "\n")
?qnorm
qnorm(0.8)
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv("PredictiveAnalytics.csv")
setwd("~/Desktop/STAT 340/discussion")
df <- read.csv("PredictiveAnalytics.csv")
n <- nrow(df)
K <- 10
Kfolds <- split( sample(1:n, n,replace=FALSE), as.factor(1:K));
Kfolds[1]
Kfolds[[1]]
Kfolds[1]
Kfolds[[1]]
for(i in 1:K){
# select the training data by removing rows from the `df` data frame.
# recall that a negative sign omits row indices
trainData <- df[-Kfolds[[i]], ]
# select the testing data by only taking the certain rows from the `df` data frame.
testData <- df[Kfolds[[i]], ]
#fit the logistic model.
# remember to use the glm function, and family="binomial". Also remembe to use the training data.
# The response variable is Turnover, and the four predictors are JS, OC, TI and Naff
model <- glm(Turnover ~ JS + OC + TI + Naff, data=trainData, family="binomial")
#Finally make predictions for the testing data.
#remember that you want type="response" to have a value between 0 and 1.
#Also simply check whether the prediction > .5
predictions <- predict(model, testData, type="response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
#The accuracy is what proportion of predictions match the actual value of Turnover
#You can use == to compare the prediction to the value of Turnoever, since TRUE has the numerical value of 1 in R.
#And you can use the mean function to calculate what proportion of these comparisons are equal; This is the accuracy.
accuracy <- mean(predicted_classes == testData$Turnover)
results[i] <- accuracy
}
results <- numeric(K)
for(i in 1:K){
# select the training data by removing rows from the `df` data frame.
# recall that a negative sign omits row indices
trainData <- df[-Kfolds[[i]], ]
# select the testing data by only taking the certain rows from the `df` data frame.
testData <- df[Kfolds[[i]], ]
#fit the logistic model.
# remember to use the glm function, and family="binomial". Also remembe to use the training data.
# The response variable is Turnover, and the four predictors are JS, OC, TI and Naff
model <- glm(Turnover ~ JS + OC + TI + Naff, data=trainData, family="binomial")
#Finally make predictions for the testing data.
#remember that you want type="response" to have a value between 0 and 1.
#Also simply check whether the prediction > .5
predictions <- predict(model, testData, type="response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
#The accuracy is what proportion of predictions match the actual value of Turnover
#You can use == to compare the prediction to the value of Turnoever, since TRUE has the numerical value of 1 in R.
#And you can use the mean function to calculate what proportion of these comparisons are equal; This is the accuracy.
accuracy <- mean(predicted_classes == testData$Turnover)
results[i] <- accuracy
}
results_df <- data.frame(Fold = 1:K, Accuracy = results)
average_accuracy <- mean(results)
average_accuracy
full_model <- glm(Turnover ~ JS + OC + TI + Naff, data = df, family = "binomial")
full_predictions <- predict(full_model, df, type = "response")
full_predicted_classes <- ifelse(full_predictions > 0.5, 1, 0)
in_sample_accuracy <- mean(full_predicted_classes == df$Turnover)
in_sample_accuracy
