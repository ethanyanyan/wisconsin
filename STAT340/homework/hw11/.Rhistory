penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > 0.5, 1, 0)
penguins.complete$actual_chinstrap <- ifelse(penguins.complete$species == "Chinstrap", 1, 0)
conf_matrix <- table(Predicted = penguins.complete$predicted_chinstrap, Actual = penguins.complete$actual_chinstrap)
conf_matrix
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])
type_1_error
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
power
1/265
3/68
65/68
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
# Test thresholds from 0 to 1 in small steps
thresholds <- seq(0, 1, by = 0.01)
for (thresh in thresholds) {
metrics <- calculate_metrics(penguins.complete, thresh)
if (metrics$type_1_error < 0.05) {  # Check if Type 1 error is below the desired level
if (metrics$power > best_power) {  # Check if this threshold provides a higher power
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
calculate_metrics <- function(data, threshold) {
data$predicted_chinstrap <- ifelse(data$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = data$predicted_chinstrap, Actual = data$actual_chinstrap)
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
# Test thresholds from 0 to 1 in small steps
thresholds <- seq(0, 1, by = 0.01)
for (thresh in thresholds) {
metrics <- calculate_metrics(penguins.complete, thresh)
if (metrics$type_1_error < 0.05) {  # Check if Type 1 error is below the desired level
if (metrics$power > best_power) {  # Check if this threshold provides a higher power
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
calculate_metrics <- function(threshold) {
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = penguins.complete$predicted_chinstrap, Actual = penguins.complete$actual_chinstrap)
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
# Test thresholds from 0 to 1 in small steps
thresholds <- seq(0, 1, by = 0.01)
for (thresh in thresholds) {
metrics <- calculate_metrics(penguins.complete, thresh)
if (metrics$type_1_error < 0.05) {  # Check if Type 1 error is below the desired level
if (metrics$power > best_power) {  # Check if this threshold provides a higher power
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
calculate_metrics <- function(threshold) {
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = penguins.complete$predicted_chinstrap, Actual = penguins.complete$actual_chinstrap)
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
# Test thresholds from 0 to 1 in small steps
thresholds <- seq(0, 1, by = 0.01)
for (thresh in thresholds) {
metrics <- calculate_metrics(thresh)
if (metrics$type_1_error < 0.05) {  # Check if Type 1 error is below the desired level
if (metrics$power > best_power) {  # Check if this threshold provides a higher power
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
calculate_metrics <- function(threshold) {
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = penguins.complete$predicted_chinstrap, Actual = penguins.complete$actual_chinstrap)
print(conf_matrix)
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
# Test thresholds from 0 to 1 in small steps
thresholds <- seq(0, 1, by = 0.01)
for (thresh in thresholds) {
metrics <- calculate_metrics(thresh)
if (metrics$type_1_error < 0.05) {  # Check if Type 1 error is below the desired level
if (metrics$power > best_power) {  # Check if this threshold provides a higher power
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
calculate_metrics <- function(threshold) {
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = factor(penguins.complete$predicted_chinstrap, levels = c(0, 1)), Actual = factor(penguins.complete$actual_chinstrap, levels = c(0, 1)))
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
# Test thresholds from 0 to 1 in small steps
thresholds <- seq(0, 1, by = 0.01)
for (thresh in thresholds) {
metrics <- calculate_metrics(thresh)
if (metrics$type_1_error < 0.05) {  # Check if Type 1 error is below the desired level
if (metrics$power > best_power) {  # Check if this threshold provides a higher power
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
# Output the best results
cat("Best Threshold:", best_threshold, "\n")
cat("Type 1 Error Rate:", best_type_1_error, "\n")
cat("Power (Sensitivity):", best_power, "\n")
calculate_metrics <- function(threshold) {
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = factor(penguins.complete$predicted_chinstrap, levels = c(0, 1)), Actual = factor(penguins.complete$actual_chinstrap, levels = c(0, 1)))
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
thresholds <- seq(0, 1, by = 0.001)
for (thresh in thresholds) {
metrics <- calculate_metrics(thresh)
if (metrics$type_1_error < 0.05) {
if (metrics$power > best_power) {
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
cat("Best Threshold:", best_threshold, "\n")
cat("Type 1 Error Rate:", best_type_1_error, "\n")
cat("Power (Sensitivity):", best_power, "\n")
calculate_metrics <- function(threshold) {
penguins.complete$predicted_chinstrap <- ifelse(penguins.complete$predicted_prob > threshold, 1, 0)
conf_matrix <- table(Predicted = factor(penguins.complete$predicted_chinstrap, levels = c(0, 1)), Actual = factor(penguins.complete$actual_chinstrap, levels = c(0, 1)))
type_1_error <- conf_matrix[2, 1] / sum(conf_matrix[, 1])  # FP / (TN + FP)
power <- conf_matrix[2, 2] / sum(conf_matrix[, 2])        # TP / (TP + FN)
return(list(type_1_error = type_1_error, power = power))
}
best_threshold <- NULL
best_power <- 0
best_type_1_error <- 1
thresholds <- seq(0, 1, by = 0.001)
for (thresh in thresholds) {
metrics <- calculate_metrics(thresh)
if (metrics$type_1_error < 0.05) {
if (metrics$power > best_power) {
best_power <- metrics$power
best_type_1_error <- metrics$type_1_error
best_threshold <- thresh
}
}
}
cat("Best Threshold:", best_threshold, "\n")
cat("Type 1 Error Rate:", best_type_1_error, "\n")
cat("Best Power:", best_power, "\n")
?qnorm
qnorm(0.8)
swiss = datasets::swiss
?datasets::swiss
head(swiss)
ggplot(swiss, aes(x=Education, y=Fertility)) +
geom_point(aes(color=Catholic), alpha=0.6) +
geom_smooth(method="lm", color="blue") +
scale_color_gradient(low="blue", high="red") +
labs(title="Fertility vs. Education",
subtitle="Colored by percentage of Catholics",
x="Education (%)",
y="Fertility") +
theme_minimal()
library(ggplot)
library(ggplot2)
ggplot(swiss, aes(x=Education, y=Fertility)) +
geom_point(aes(color=Catholic), alpha=0.6) +
geom_smooth(method="lm", color="blue") +
scale_color_gradient(low="blue", high="red") +
labs(title="Fertility vs. Education",
subtitle="Colored by percentage of Catholics",
x="Education (%)",
y="Fertility") +
theme_minimal()
full_model <- lm(Fertility ~ ., data = swiss)
reduced_model <- lm(Fertility ~ Education + Catholic + Infant.Mortality, data = swiss)
interaction_model <- lm(Fertility ~ Education * Catholic + Agriculture + Examination, data = swiss)
summary(full_model)
summary(reduced_model)
summary(interaction_model)
full_model <- lm(Fertility ~ ., data = swiss)
reduced_model <- lm(Fertility ~ Education + Catholic + Infant.Mortality + Agriculture, data = swiss)
interaction_model <- lm(Fertility ~ Education * Catholic + Agriculture + Examination, data = swiss)
summary(full_model)
summary(reduced_model)
summary(interaction_model)
set.seed(1)
library(caret)
set.seed(1)
full_model_cv <- train(Fertility ~ ., method="lm", data=swiss,
trControl = trainControl(method="cv", number=5))
print(full_model_cv)
reduced_model_cv <- train(Fertility ~ Education + Catholic + Infant.Mortality + Agriculture,
method="lm", data=swiss,
trControl = trainControl(method="cv", number=5))
print(reduced_model_cv)
interaction_model_cv <- train(Fertility ~ Education * Catholic + Agriculture + Examination,
method="lm", data=swiss,
trControl = trainControl(method="cv", number=5))
print(interaction_model_cv)
# Setting control with repeated CV
repeated_cv_controls <- trainControl(method="repeatedcv", number=5, repeats=3)
full_model_repeated_cv <- train(Fertility ~ ., method="lm", data=swiss,
trControl = repeated_cv_controls)
print(full_model_repeated_cv)
reduced_model_repeated_cv <- train(Fertility ~ Education + Catholic + Infant.Mortality + Agriculture,
method="lm", data=swiss,
trControl = repeated_cv_controls)
print(reduced_model_repeated_cv)
interaction_model_repeated_cv <- train(Fertility ~ Education * Catholic + Agriculture + Examination,
method="lm", data=swiss,
trControl = repeated_cv_controls)
print(interaction_model_repeated_cv)
Auto <- read.csv("Auto.csv", stringsAsFactors = TRUE)
setwd("~/Desktop/STAT 340/homework/hw11")
Auto <- read.csv("Auto.csv", stringsAsFactors = TRUE)
# id: model id
# object: regsubsets object
# data: data used to fit regsubsets
# outcome: outcome variable
get_model_formula <- function(id, object, outcome){
# get models data
models <- summary(object)$which[id,-1]
# Get outcome variable
#form <- as.formula(object$call[[2]])
#outcome <- all.vars(form)[1]
# Get model predictors
predictors <- names(which(models == TRUE))
predictors <- paste(predictors, collapse = "+")
# Build model formula
as.formula(paste0(outcome, "~", predictors))
}
library(leaps)
getModels <- function (dataset, responseVar){
models <- regsubsets(reformulate(".",responseVar), data = Auto.subset, nvmax = ncol(dataset)-2);
modelList <- list("formula")
nModels <- length(summary(models))-1
for(i in 1:nModels){
modelList[[i]] <- get_model_formula(i, models, responseVar)
}
return(modelList)
}
sampleSize <- 15  #You should edit this number
NMC <- 50 #number of replications of this simulation
nFolds <- 5  #k=5 for 5-fold CV
nModels <- 7 # we will look at a maximum model size (# predictors) of 7.
errors <- data.frame('fold' = as.factor(rep(1:nFolds, nModels*NMC)),
'rep' = rep(1:NMC, each=nFolds*nModels),
'model' = rep(1:nModels, rep(nFolds, nModels)),
'mse' = rep(0, nModels*nFolds*NMC))
for(k in 1:NMC){
Auto.subset <- Auto[sample(nrow(Auto), sampleSize),]
modelList <- getModels(Auto.subset, "mpg")
#Cross Validation
folds <- split(sample(1:nrow(Auto.subset)), as.factor(1:nFolds))
for(i in 1:nFolds){
validation <- Auto.subset[folds[[i]],]
training <- Auto.subset[-folds[[i]],]
for(j in 1:nModels){
fit <- lm(modelList[[j]], data=training)
predictions <- predict(fit, newdata = validation)
errors[errors$rep==k & errors$fold==i  & errors$model ==j, 'mse'] = mean((predictions-validation$mpg)^2)
}
}
}
avg <- aggregate(.~model, data=errors, FUN="mean")
plot(y=sqrt(avg$mse), x=avg$model, xlab="model", ylab="root mean square error", type="l", main="Comparison of Model Error")
sampleSize <- 60  #You should edit this number
NMC <- 50 #number of replications of this simulation
nFolds <- 5  #k=5 for 5-fold CV
nModels <- 7 # we will look at a maximum model size (# predictors) of 7.
errors <- data.frame('fold' = as.factor(rep(1:nFolds, nModels*NMC)),
'rep' = rep(1:NMC, each=nFolds*nModels),
'model' = rep(1:nModels, rep(nFolds, nModels)),
'mse' = rep(0, nModels*nFolds*NMC))
for(k in 1:NMC){
Auto.subset <- Auto[sample(nrow(Auto), sampleSize),]
modelList <- getModels(Auto.subset, "mpg")
#Cross Validation
folds <- split(sample(1:nrow(Auto.subset)), as.factor(1:nFolds))
for(i in 1:nFolds){
validation <- Auto.subset[folds[[i]],]
training <- Auto.subset[-folds[[i]],]
for(j in 1:nModels){
fit <- lm(modelList[[j]], data=training)
predictions <- predict(fit, newdata = validation)
errors[errors$rep==k & errors$fold==i  & errors$model ==j, 'mse'] = mean((predictions-validation$mpg)^2)
}
}
}
avg <- aggregate(.~model, data=errors, FUN="mean")
plot(y=sqrt(avg$mse), x=avg$model, xlab="model", ylab="root mean square error", type="l", main="Comparison of Model Error")
sampleSize <- 15  #You should edit this number
NMC <- 50 #number of replications of this simulation
nFolds <- 5  #k=5 for 5-fold CV
nModels <- 7 # we will look at a maximum model size (# predictors) of 7.
errors <- data.frame('fold' = as.factor(rep(1:nFolds, nModels*NMC)),
'rep' = rep(1:NMC, each=nFolds*nModels),
'model' = rep(1:nModels, rep(nFolds, nModels)),
'mse' = rep(0, nModels*nFolds*NMC))
for(k in 1:NMC){
Auto.subset <- Auto[sample(nrow(Auto), sampleSize),]
modelList <- getModels(Auto.subset, "mpg")
#Cross Validation
folds <- split(sample(1:nrow(Auto.subset)), as.factor(1:nFolds))
for(i in 1:nFolds){
validation <- Auto.subset[folds[[i]],]
training <- Auto.subset[-folds[[i]],]
for(j in 1:nModels){
fit <- lm(modelList[[j]], data=training)
predictions <- predict(fit, newdata = validation)
errors[errors$rep==k & errors$fold==i  & errors$model ==j, 'mse'] = mean((predictions-validation$mpg)^2)
}
}
}
avg <- aggregate(.~model, data=errors, FUN="mean")
plot(y=sqrt(avg$mse), x=avg$model, xlab="model", ylab="root mean square error", type="l", main="Comparison of Model Error")
sampleSize <- 200  #You should edit this number
NMC <- 50 #number of replications of this simulation
nFolds <- 5  #k=5 for 5-fold CV
nModels <- 7 # we will look at a maximum model size (# predictors) of 7.
errors <- data.frame('fold' = as.factor(rep(1:nFolds, nModels*NMC)),
'rep' = rep(1:NMC, each=nFolds*nModels),
'model' = rep(1:nModels, rep(nFolds, nModels)),
'mse' = rep(0, nModels*nFolds*NMC))
for(k in 1:NMC){
Auto.subset <- Auto[sample(nrow(Auto), sampleSize),]
modelList <- getModels(Auto.subset, "mpg")
#Cross Validation
folds <- split(sample(1:nrow(Auto.subset)), as.factor(1:nFolds))
for(i in 1:nFolds){
validation <- Auto.subset[folds[[i]],]
training <- Auto.subset[-folds[[i]],]
for(j in 1:nModels){
fit <- lm(modelList[[j]], data=training)
predictions <- predict(fit, newdata = validation)
errors[errors$rep==k & errors$fold==i  & errors$model ==j, 'mse'] = mean((predictions-validation$mpg)^2)
}
}
}
avg <- aggregate(.~model, data=errors, FUN="mean")
plot(y=sqrt(avg$mse), x=avg$model, xlab="model", ylab="root mean square error", type="l", main="Comparison of Model Error")
sampleSize <- 200  #You should edit this number
NMC <- 50 #number of replications of this simulation
nFolds <- 5  #k=5 for 5-fold CV
nModels <- 7 # we will look at a maximum model size (# predictors) of 7.
errors <- data.frame('fold' = as.factor(rep(1:nFolds, nModels*NMC)),
'rep' = rep(1:NMC, each=nFolds*nModels),
'model' = rep(1:nModels, rep(nFolds, nModels)),
'mse' = rep(0, nModels*nFolds*NMC))
for(k in 1:NMC){
Auto.subset <- Auto[sample(nrow(Auto), sampleSize),]
modelList <- getModels(Auto.subset, "mpg")
#Cross Validation
folds <- split(sample(1:nrow(Auto.subset)), as.factor(1:nFolds))
for(i in 1:nFolds){
validation <- Auto.subset[folds[[i]],]
training <- Auto.subset[-folds[[i]],]
for(j in 1:nModels){
fit <- lm(modelList[[j]], data=training)
predictions <- predict(fit, newdata = validation)
errors[errors$rep==k & errors$fold==i  & errors$model ==j, 'mse'] = mean((predictions-validation$mpg)^2)
}
}
}
avg <- aggregate(.~model, data=errors, FUN="mean")
plot(y=sqrt(avg$mse), x=avg$model, xlab="model", ylab="root mean square error", type="l", main="Comparison of Model Error")
simulate.data <- function(n=40){
X1 <- rnorm(n, 3, 1)
X2 <- rnorm(n, 1, .5)
eps <- rnorm(n, 0, 2)
Y <- 4 + 5*X1 + 8*X2 + eps
return(data.frame(Y,X1,X2))
}
NMC <- 1000
nUnseen <- 1000
Ehat <- 0 #an empty vector to store estimated
for(i in 1:NMC){
# generate a sample of size 40
sample <- simulate.data()
#fit the linear model with X1 and X2 as predictors
model <- lm(Y ~ X1 + X2, data = sample)
#simulate unseen data
unseen_data <- simulate.data(n = nUnseen)
#calculate the square root of the average squared error on the out of sample data points
predictions <- predict(model, newdata = unseen_data)
#store this in Ehat[i]
Ehat[i] <- sqrt(mean((unseen_data$Y - predictions)^2))
}
(modelError <- mean(Ehat))
kfoldCV <- function(K, formula, dataset, responseVar){
#idx is a shuffled vector of row numbers
idx <- sample(1:nrow(dataset))
#folds partitions the row indices
folds <- suppressWarnings(split(idx, as.factor(1:K)))
#an empty vector to hold estimated errors
errors <- vector("numeric", K)
for(k in 1:K){
#split the data into training and testing sets
training <- dataset[-folds[[k]],]
testing <- dataset[folds[[k]],]
#go through each model and estimate MSE
#fit the model to the training data
fit <- lm(formula = formula, data=training)
#calculate the sqrt of average squared error on the testing data
errors[k] <- sqrt (mean((predict(fit, newdata=testing)-testing[,responseVar])^2))
}
return(mean(errors))
}
NMC <- 50
Ks <- c(2,3,4,5,6,8,10,15,20,30,40)
nK <- length(Ks)
formula <- reformulate(c("X1","X2"),"Y")
errors <- data.frame('replicate'=rep(1:NMC, each=nK),
'k' = rep(Ks, NMC),
'error' = rep(0, NMC*nK))
for(i in 1:NMC){
myData <- simulate.data(40)
for(k in Ks){
errors[errors$replicate==i & errors$k==k, 'error'] <- kfoldCV(k, formula, myData, 'Y')
}
}
averageErrors <- aggregate(error ~k, data=errors, FUN="mean")
plot(error ~ k, data=errors, col=rgb(0,0,0,.5))
lines(error ~ k, data=averageErrors, col="red", lwd=3)
abline(h=modelError)
vars <- aggregate(error ~k, data=errors, FUN="var")$error
bias2 <- (averageErrors$error-2.07)^2
errors$errorsq <- (errors$error-2.07)^2
mse <- aggregate(errorsq ~ k, data=errors, FUN="mean")$errorsq
plot(x=Ks, y=vars, ylim=c(0,max(mse)), type="l", ylab="")
lines(x=Ks, y=bias2, lty=2, col="blue")
lines(x=Ks, y=mse, lty=3, lwd=2, col="red")
library(ISLR)
Carseats = ISLR::Carseats
# you should read the help page by running ?Carseats
# we can also peek at the data frame before using it
str(Carseats)
head(Carseats)
numericData <- Carseats %>% select_if(is.numeric)
library(dplyr)
library(corrplot)
ggplot(Carseats, aes(x=Price, y=Sales)) +
geom_point(aes(color=Urban, shape=ShelveLoc), alpha=0.5) +
geom_smooth(method="lm") +
labs(title="Sales vs Price", x="Price", y="Sales")
ggplot(Carseats, aes(x=Advertising, y=Sales)) +
geom_point(aes(color=Urban, shape=ShelveLoc), alpha=0.5) +
geom_smooth(method="lm") +
labs(title="Sales vs Advertising", x="Advertising", y="Sales")
ggplot(Carseats, aes(x=ShelveLoc, y=Sales)) +
geom_boxplot(aes(fill=ShelveLoc)) +
labs(title="Sales across Shelf Locations", x="Shelf Location", y="Sales")
summary(final_model)
library(MASS)
full_model <- lm(Sales ~ . + Price:ShelveLoc + Advertising:Urban, data=Carseats)
summary(full_model)
step_model <- stepAIC(full_model, direction="both")
summary(step_model)
final_model <- lm(Sales ~ Price + Income + ShelveLoc + CompPrice + Advertising + Age, data=Carseats)
summary(final_model)
summary(final_model)
?Carseats
train_indices <- sample(1:nrow(Carseats), 0.7 * nrow(Carseats))
train_data <- Carseats[train_indices, ]
test_data <- Carseats[-train_indices, ]
fitted_model <- lm(Sales ~ Price + Income + ShelveLoc + CompPrice + Advertising + Age, data=train_data)
predictions <- predict(fitted_model, newdata=test_data)
mse <- mean((test_data$Sales - predictions)^2)
mse
par(mfrow=c(2,2))
plot(fitted_model)
