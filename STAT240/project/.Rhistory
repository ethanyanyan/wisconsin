ggplot(max_temps, aes(x = tmax)) +
geom_density(fill = "blue", alpha = 0.5) +
geom_vline(xintercept = summary$x_bar, color = "red", linetype = "dashed") +
geom_vline(xintercept = summary$x_bar+summary$sd, color = "black", linetype = "dashed") +
geom_vline(xintercept = summary$x_bar-summary$sd, color = "black", linetype = "dashed") +
xlab("High Temperature (Â°F)") +
xlim(0, 120) +
ylab("Density") +
ggtitle("Density of High Temperatures on April 14th (2003-2022)")
# Chunk 4
q_normal = qnorm(0.975)
q_t = qt(0.975, df = 19)
q_normal_left = qnorm(0.025)
q_t_left = qt(0.025, df = 19)
x_values = seq(-4, 4, length.out = 100)
# Create a data frame for ggplot
plot_data = data.frame(
x = x_values,
dnorm = dnorm(x_values),
dt = dt(x_values, df = 19)
)
ggplot(plot_data, aes(x)) +
geom_line(aes(y = dnorm), color = "blue") +
geom_line(aes(y = dt), color = "red") +
geom_vline(xintercept = q_normal, color = "blue", linetype = "dashed") +
geom_vline(xintercept = q_t, color = "red", linetype = "dashed") +
geom_norm_fill(mu = 0, sigma = 1, a = q_normal, b = NULL, fill = "blue", alpha = 0.5) +
geom_t_fill(df = 19, a = q_t, b = NULL, fill = "red", alpha = 0.5) +
geom_norm_fill(mu = 0, sigma = 1, a = NULL, b = q_normal_left, fill = "blue", alpha = 0.5) +
geom_t_fill(df = 19, a = NULL, b = q_t_left, fill = "red", alpha = 0.5) +
xlab("Value") + ylab("Density") +
ggtitle("Standard Normal vs. T-Distribution (df = 19)") +
labs(subtitle = "Density functions with 0.975 quantiles (dashed lines). Tails below 0.025 and above 0.975 quantiles shaded")
# Chunk 5
temps_sd = (summary$sd)/sqrt(summary$n)
z = qt(0.975, df=19)
ci = (summary$x_bar) + c(-1,1)*z*temps_sd
round(ci,3)
# Chunk 6
(t.test(max_temps))$conf.int
# Chunk 7
## Assuming the null
mu0 = 55.6
summary = summary %>%
mutate(tstat = (x_bar - mu0)/(sd/sqrt(n)))
tstat = summary %>%
pull(tstat)
summary
tstat
# Chunk 8
# Calculate the p-value for a two-tailed test
pvalue = 2 * pt(-abs(tstat), df = summary$n - 1)
# Display the p-value
pvalue
# Chunk 9
t.test(max_temps, mu=55.6)
# Chunk 10
filtered_weather = weather %>%
filter((year(date) >= 1903 & year(date) <= 1922 | year(date) >= 2003 & year(date) <= 2022) &
snow >= 1 &
month(date) >= 1 & month(date) <= 6) %>%
group_by(year(date)) %>%
filter(date == max(date)) %>%
mutate(yday = yday(date)) %>%
mutate(period = ifelse(year(date) >= 1903 & year(date) <= 1922, "early 1900s", "early 2000s"))
print(filtered_weather, n=10)
# Chunk 11
sample_details = filtered_weather %>%
group_by(period) %>%
summarise(n = n(),
mean = mean(yday),
sd = sd(yday))
sample_details
# Chunk 12
ggplot(filtered_weather, aes(x = period, y = yday, fill = period)) +
geom_boxplot(coef = Inf, alpha = 0.5) +
geom_point(position = position_jitter(width=0.3, height=0)) +
labs(title = "Comparison of Latest Snow Dates between Early 1900s and Early 2000s",
x = "Period",
y = "Latest Day of the Year with Snow") +
theme_minimal()
# Chunk 13
s1_data = filtered_weather %>% filter(period == 'early 1900s') %>% pull(yday)
s2_data = filtered_weather %>% filter(period == 'early 2000s') %>% pull(yday)
(t.test(s1_data, s2_data))$conf.int
# Chunk 14
t.test(s1_data, s2_data)
# Chunk 15
bm = read_csv('./../../data/boston-marathon-data.csv')
bm_summary = bm %>%
filter(Age_Range == '35-39' & Sex == 'male' & Year == '2010') %>%
summarise(
n = n(),
mean = mean(Time, na.rm = TRUE),
sd = sd(Time, na.rm = TRUE),
quantile10 = quantile(Time, 0.10, na.rm = TRUE),
quantile25 = quantile(Time, 0.25, na.rm = TRUE),
median = quantile(Time, 0.50, na.rm = TRUE),
quantile75 = quantile(Time, 0.75, na.rm = TRUE),
quantile90 = quantile(Time, 0.90, na.rm = TRUE)
)
bm_summary
# Chunk 16
bm_filtered = bm %>%
filter(Age_Range == '35-39' & Sex == 'male' & Year == 2010)
ggplot(bm_filtered, aes(x = Time)) +
geom_density(fill = "blue", alpha = 0.7) +
labs(title = "Density Plot of Finish Times for Male Runners Aged 35-39 in 2010 Boston Marathon",
x = "Finish Time (minutes)",
y = "Density") +
theme_minimal()
# Chunk 17
ci_calc = bm_summary %>%
mutate(se = sd/sqrt(n),
tmult = qt(0.975, n-1),
me = tmult*se,
low = mean - me,
high = mean + me)
ci_calc %>%
print(width = Inf)
ci = ci_calc %>%
select(low, high)
ci
# Chunk 18
(t.test(bm_filtered$Time))$conf.int
# Chunk 19
bm_both = bm %>%
filter(Age_Range == '35-39' & Sex == 'male' & (Year == 2010 | Year == 2011)) %>%
mutate(year = as.character(Year))
ggplot(bm_both, aes(x = year, y = Time, fill = year)) +
geom_boxplot(coef = Inf, alpha = 0.5) +
geom_point(position = position_jitter(width=0.3, height=0)) +
xlab("Year") +
ylab("Finish Time") +
ggtitle("Comparison of Male Boston Marathon Finishers Aged 35-39 in 2010 and 2011") +
theme_minimal()
# Chunk 20
bm_summary = bm_both %>%
group_by(year) %>%
summarise(
n = n(),
mean = mean(Time),
sd = sd(Time)
)
bm_summary
# Chunk 21
# Subset data for the two years
data_2010 = bm %>% filter(Age_Range == '35-39', Sex == 'male', Year == 2010) %>% pull(Time)
data_2011 = bm %>% filter(Age_Range == '35-39', Sex == 'male', Year == 2011) %>% pull(Time)
# Conduct two-sample t-test
t_test_result = t.test(data_2010, data_2011)
t_test_result
dof = 3608.6 #t.test(x, y)$parameter
mean_x=mean(data_2010)
s_x = sd(data_2010)
n_x = length(data_2010)
mean_y = mean(data_2011)
s_y = sd(data_2011)
n_y = length(data_2011)
ci = mean_x - mean_y + c(-1,1)*qt(0.975, dof)*sqrt(s_x^2/n_x + s_y^2/n_y)
ci
filtered_weather = weather %>%
filter((year(date) >= 1903 & year(date) <= 1922 | year(date) >= 2003 & year(date) <= 2022) &
snow >= 1 &
month(date) >= 1 & month(date) <= 6) %>%
group_by(year(date)) %>%
filter(date == max(date)) %>%
mutate(yday = yday(date)) %>%
mutate(period = ifelse(year(date) >= 1903 & year(date) <= 1922, "early 1900s", "early 2000s"))
print(filtered_weather, n=10)
filtered_weather = weather %>%
filter((year(date) >= 1903 & year(date) <= 1922 | year(date) >= 2003 & year(date) <= 2022) &
snow >= 1 &
month(date) >= 1 & month(date) <= 6) %>%
drop_na(snow) %>%
group_by(year(date)) %>%
filter(date == max(date)) %>%
mutate(yday = yday(date)) %>%
mutate(period = ifelse(year(date) >= 1903 & year(date) <= 1922, "early 1900s", "early 2000s"))
print(filtered_weather, n=10)
View(weather)
filtered_weather = weather %>%
filter((year(date) >= 1903 & year(date) <= 1922 | year(date) >= 2003 & year(date) <= 2022) &
snow >= 1 &
month(date) >= 1 & month(date) <= 6) %>%
drop_na(snow) %>%
group_by(year(date)) %>%
filter(date == max(date)) %>%
mutate(yday = yday(date)) %>%
mutate(period = ifelse(year(date) >= 1903 & year(date) <= 1922, "early 1900s", "early 2000s"))
print(filtered_weather, n=10)
# Chunk 1
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
warning = FALSE, error = TRUE, fig.height = 3)
library(tidyverse)
library(lubridate)
# Load the dataset
raw_data <- read_csv("./../../data/Electric_Vehicle_Title_and_Registration_Activity.csv")
# Chunk 2
filtered_data <- raw_data %>%
rename(
vin = `VIN (1-10)`,
vehicle_id = `DOL Vehicle ID`,
model_year = `Model Year`,
model = Model,
make = Make,
new_or_used = `New or Used Vehicle`,
transaction_date = `DOL Transaction Date`,
county = County,
city = City,
state_of_residence = `State of Residence`,
postal_code = `Postal Code`
) %>%
select(vin, vehicle_id, model_year, model, make, new_or_used, transaction_date, county, city, state_of_residence, postal_code) %>%
mutate(year = as.integer(substring(transaction_date, nchar(transaction_date) - 3, nchar(transaction_date))))
# Grouping by year and summarizing the number of registrations
yearly_registrations <- filtered_data %>%
group_by(year) %>%
summarize(new_registrations = n())
# Chunk 3
# Plotting the data
ggplot(yearly_registrations, aes(x = year, y = new_registrations)) +
geom_line() +
geom_vline(xintercept = 2019, linetype = "dashed", color = "red") +
annotate("text", x = 2019, y = max(yearly_registrations$new_registrations), label = "HB 2042 Enacted", vjust = -1) +
labs(title = "Electric Vehicle Registrations Over the Years in Washington State",
subtitle = "Years 2010 to 2023",
x = "Year",
y = "Number of Registrations") +
theme_minimal()
# Chunk 4
county_summary <- filtered_data %>%
filter(year %in% c(2020, 2023)) %>%
group_by(county, year) %>%
summarize(registrations = n(), .groups = 'drop') %>%
pivot_wider(names_from = year, values_from = registrations)
county_summary <- county_summary %>%
mutate(
absolute_increase = `2023` - `2020`,
percentage_increase = (`2023` - `2020`) / `2020` * 100
)
# Define a weighting factor (e.g., median of absolute increases)
weighting_factor <- median(county_summary$absolute_increase, na.rm = TRUE)
# Compute weighted increase
county_summary <- county_summary %>%
mutate(weighted_increase = percentage_increase + (absolute_increase / weighting_factor))
# Rank counties based on weighted increase
top_counties_weighted <- county_summary %>%
arrange(desc(weighted_increase)) %>%
head(5)
top_5_counties_names <- top_counties_weighted$county
top_5_data <- filtered_data %>%
filter(county %in% top_5_counties_names, year %in% c(2020, 2021, 2022, 2023)) %>%
group_by(county, year) %>%
summarize(registrations = n(), .groups = 'drop')
# Create the time series plot
ggplot(top_5_data, aes(x = year, y = registrations, group = county, color = county)) +
geom_line() +
geom_point() +
labs(title = "EV Registrations in Top 5 Counties with Highest Weighted Increase (2020-2023)",
x = "Year", y = "Number of Registrations",
color = "County") +
theme_minimal()
# Chunk 5
# Calculating year-on-year increase
table_5_data <- top_5_data %>%
group_by(county) %>%
mutate(year_on_year_increase = (registrations / lag(registrations) - 1) * 100) %>%
na.omit()  # Remove NA values created by lag function
# Computing the average increase
average_increase_table <- table_5_data %>%
group_by(county) %>%
summarize(average_year_on_year_increase = mean(year_on_year_increase, na.rm = TRUE)) %>%
arrange(desc(average_year_on_year_increase))
# Display the table
print(average_increase_table)
View(raw_data)
View(raw_data)
pop_data <- read_csv("./../../data/wa_pop_by_county")
setwd("~/Desktop/STAT 240/homework/project")
# Chunk 1
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
warning = FALSE, error = TRUE, fig.height = 3)
library(tidyverse)
library(lubridate)
# Load the dataset
raw_data <- read_csv("./../../data/Electric_Vehicle_Title_and_Registration_Activity.csv")
# Chunk 2
filtered_data <- raw_data %>%
rename(
vin = `VIN (1-10)`,
vehicle_id = `DOL Vehicle ID`,
model_year = `Model Year`,
model = Model,
make = Make,
new_or_used = `New or Used Vehicle`,
transaction_date = `DOL Transaction Date`,
county = County,
city = City,
state_of_residence = `State of Residence`,
postal_code = `Postal Code`
) %>%
select(vin, vehicle_id, model_year, model, make, new_or_used, transaction_date, county, city, state_of_residence, postal_code) %>%
mutate(year = as.integer(substring(transaction_date, nchar(transaction_date) - 3, nchar(transaction_date))))
# Grouping by year and summarizing the number of registrations
yearly_registrations <- filtered_data %>%
group_by(year) %>%
summarize(new_registrations = n())
# Chunk 3
# Plotting the data
ggplot(yearly_registrations, aes(x = year, y = new_registrations)) +
geom_line() +
geom_vline(xintercept = 2019, linetype = "dashed", color = "red") +
annotate("text", x = 2019, y = max(yearly_registrations$new_registrations), label = "HB 2042 Enacted", vjust = -1) +
labs(title = "Electric Vehicle Registrations Over the Years in Washington State",
subtitle = "Years 2010 to 2023",
x = "Year",
y = "Number of Registrations") +
theme_minimal()
# Chunk 4
county_summary <- filtered_data %>%
filter(year %in% c(2020, 2023)) %>%
group_by(county, year) %>%
summarize(registrations = n(), .groups = 'drop') %>%
pivot_wider(names_from = year, values_from = registrations)
county_summary <- county_summary %>%
mutate(
absolute_increase = `2023` - `2020`,
percentage_increase = (`2023` - `2020`) / `2020` * 100
)
# Define a weighting factor (e.g., median of absolute increases)
weighting_factor <- median(county_summary$absolute_increase, na.rm = TRUE)
# Compute weighted increase
county_summary <- county_summary %>%
mutate(weighted_increase = percentage_increase + (absolute_increase / weighting_factor))
# Rank counties based on weighted increase
top_counties_weighted <- county_summary %>%
arrange(desc(weighted_increase)) %>%
head(5)
top_5_counties_names <- top_counties_weighted$county
top_5_data <- filtered_data %>%
filter(county %in% top_5_counties_names, year %in% c(2020, 2021, 2022, 2023)) %>%
group_by(county, year) %>%
summarize(registrations = n(), .groups = 'drop')
# Create the time series plot
ggplot(top_5_data, aes(x = year, y = registrations, group = county, color = county)) +
geom_line() +
geom_point() +
labs(title = "EV Registrations in Top 5 Counties with Highest Weighted Increase (2020-2023)",
x = "Year", y = "Number of Registrations",
color = "County") +
theme_minimal()
# Chunk 5
# Calculating year-on-year increase
table_5_data <- top_5_data %>%
group_by(county) %>%
mutate(year_on_year_increase = (registrations / lag(registrations) - 1) * 100) %>%
na.omit()  # Remove NA values created by lag function
# Computing the average increase
average_increase_table <- table_5_data %>%
group_by(county) %>%
summarize(average_year_on_year_increase = mean(year_on_year_increase, na.rm = TRUE)) %>%
arrange(desc(average_year_on_year_increase))
# Display the table
print(average_increase_table)
# Chunk 6
pop_data <- read_csv("./../../data/wa_pop_by_county")
pop_data <- read_csv("./../../data/wa_pop_by_county")
pop_data <- read_csv("./../../data/wa_pop_by_county.csv")
View(pop_data)
pop_data <- read_csv("./../../data/wa_pop_by_county.csv")
# Calculate the average population for each county in 2020, 2021, 2022, 2023
average_pop_data <- pop_data %>%
group_by(COUNTY) %>%
summarize(Average_Population = mean(c(POP_2020, POP_2021, POP_2022, POP_2023), na.rm = TRUE))
# Determine the median of the average populations
median_population <- median(average_pop_data$Average_Population, na.rm = TRUE)
# Create a new variable in your dataset that indicates whether a county's average population is above or below the median
pop_data <- pop_data %>%
left_join(average_pop_data, by = "COUNTY") %>%
mutate(Population_Group = ifelse(Average_Population > median_population, "Above Median", "Below Median"))
View(pop_data)
View(county_summary)
pop_data <- read_csv("./../../data/wa_pop_by_county.csv")
pop_data <- pop_data %>%
rename(
county = COUNTY,
pop_2020 = POP_2020,
pop_2021 = POP_2021,
pop_2022 = POP_2022,
pop_2023 = POP_2023
)
# Calculate the average population for each county in 2020, 2021, 2022, 2023
average_pop_data <- pop_data %>%
group_by(county) %>%
summarize(average_population = mean(c(pop_2020, pop_2021, pop_2022, pop_2023), na.rm = TRUE))
# Determine the median of the average populations
median_population <- median(average_pop_data$average_population, na.rm = TRUE)
# Create a new variable in your dataset that indicates whether a county's average population is above or below the median
pop_data <- pop_data %>%
left_join(average_pop_data, by = "county") %>%
mutate(population_group = ifelse(average_population > median_population, "Above Median", "Below Median"))
View(pop_data)
pop_data <- read_csv("./../../data/wa_pop_by_county.csv")
pop_data <- pop_data %>%
rename(
county = COUNTY,
pop_2020 = POP_2020,
pop_2021 = POP_2021,
pop_2022 = POP_2022,
pop_2023 = POP_2023
)
# Calculate the average population for each county in 2020, 2021, 2022, 2023
average_pop_data <- pop_data %>%
group_by(county) %>%
summarize(average_population = mean(c(pop_2020, pop_2021, pop_2022, pop_2023), na.rm = TRUE))
# Determine the median of the average populations
median_population <- median(average_pop_data$average_population, na.rm = TRUE)
# Create a new variable in your dataset that indicates whether a county's average population is above or below the median
pop_data <- pop_data %>%
left_join(average_pop_data, by = "county") %>%
mutate(population_group = ifelse(average_population > median_population, "Above Median", "Below Median"))
merged_data <- merge(county_summary, pop_data, by = "county")
View(merged_data)
merged_data <- merge(county_summary, pop_data, by = "county")
above_median_data <- subset(merged_data, Population_Group == "Above Median")$weighted_increase
merged_data <- merge(county_summary, pop_data, by = "county")
above_median_data <- subset(merged_data, population_Group == "Above Median")$weighted_increase
merged_data <- merge(county_summary, pop_data, by = "county")
above_median_data <- subset(merged_data, population_group == "Above Median")$weighted_increase
below_median_data <- subset(merged_data, population_group == "Below Median")$weighted_increase
# Check for normality (e.g., using Shapiro-Wilk test)
shapiro.test(above_median_data)
shapiro.test(below_median_data)
# Check for equal variances (e.g., using Levene's test)
leveneTest(above_median_data, below_median_data)
merged_data <- merge(county_summary, pop_data, by = "county")
above_median_data <- subset(merged_data, population_group == "Above Median")$weighted_increase
below_median_data <- subset(merged_data, population_group == "Below Median")$weighted_increase
# Check for normality (e.g., using Shapiro-Wilk test)
shapiro.test(above_median_data)
shapiro.test(below_median_data)
# Perform the t-test
# Use var.equal = TRUE if variances are equal, else use var.equal = FALSE
t_test_result <- t.test(above_median_data, below_median_data, var.equal = TRUE)  # or FALSE for Welch's t-test
View(t_test_result)
merged_data <- merge(county_summary, pop_data, by = "county")
above_median_data <- subset(merged_data, population_group == "Above Median")$weighted_increase
below_median_data <- subset(merged_data, population_group == "Below Median")$weighted_increase
# Check for normality (e.g., using Shapiro-Wilk test)
shapiro.test(above_median_data)
shapiro.test(below_median_data)
# Perform the t-test
# Use var.equal = TRUE if variances are equal, else use var.equal = FALSE
t_test_result <- t.test(above_median_data, below_median_data)
t_test_result
View(merged_data)
merged_data <- merge(county_summary, pop_data, by = "county")
above_median_data <- subset(merged_data, population_group == "Above Median")$percentage_increase
below_median_data <- subset(merged_data, population_group == "Below Median")$percentage_increase
t.test(above_median_data, below_median_data)
View(merged_data)
View(county_summary)
county_summary$percentage_increase %>%
drop_na() %>%
mean()
drop_na(county_summary$percentage_increase)
county_summary %>% drop_na()
county_summary %>% drop_na() %>% summarise(mean = mean(percentage_increase))
ggplot(yearly_registrations, aes(x = new_registrations)) +
geom_histogram(bins = 30, fill = "blue", color = "black") +
labs(title = "Histogram of Yearly EV Registrations",
x = "Number of Registrations",
y = "Frequency") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
library(tidyverse)
library(lubridate)
source("../../scripts/viridis.R")
source("../../scripts/ggprob.R")
## Read and transform the Madison weather data
mw_orig = read_csv("../../data/madison-weather-official-1869-2022.csv")
mw = mw_orig %>%
mutate(year = year(date),
month = month(date, label = TRUE),
.after = date) %>%
mutate(year1 = case_when(
month > "Jun" ~ year,
month < "Jul" ~ year - 1),
year2 = year1 + 1,
winter = str_c(year1, "-", year2),
.before = year)
## add the winter variables and filter
mw_winter = mw %>%
filter(month %in% c("Nov", "Dec", "Jan", "Feb")) %>%
select(-prcp, -contains("snow")) %>%
drop_na() %>%
filter(winter >= "1869-1870" & winter <= "2019-2020") %>%
group_by(winter, year1) %>%
summarize(tavg = mean(tavg))
SE_slope <- 0.00668
slope <- 0.01934
df <- 149
t_quantile <- qt(1 - 0.025, df)
ME <- t_quantile * SE_slope
CI_lower <- slope - ME
CI_upper <- slope + ME
ME_rounded <- round(ME, 2)
CI_lower_rounded <- round(CI_lower, 2)
CI_upper_rounded <- round(CI_upper, 2)
cat("Margin of Error:", ME_rounded, "\n")
cat("95% Confidence Interval: (", CI_lower_rounded, ", ", CI_upper_rounded, ")", "\n")
